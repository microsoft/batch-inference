# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: batch_inference/aio/remote_model_host/model_host.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder

# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n6batch_inference/aio/remote_model_host/model_host.proto"\x1b\n\x07Request\x12\x10\n\x08ndarrays\x18\x01 \x01(\x0c"\x1c\n\x08Response\x12\x10\n\x08ndarrays\x18\x01 \x01(\x0c\x32-\n\tModelHost\x12 \n\x07predict\x12\x08.Request\x1a\t.Response"\x00\x62\x06proto3',
)

_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
_builder.BuildTopDescriptorsAndMessages(
    DESCRIPTOR,
    "batch_inference.aio.remote_model_host.model_host_pb2",
    globals(),
)
if _descriptor._USE_C_DESCRIPTORS == False:
    DESCRIPTOR._options = None
    _REQUEST._serialized_start = 58
    _REQUEST._serialized_end = 85
    _RESPONSE._serialized_start = 87
    _RESPONSE._serialized_end = 115
    _MODELHOST._serialized_start = 117
    _MODELHOST._serialized_end = 162
# @@protoc_insertion_point(module_scope)
